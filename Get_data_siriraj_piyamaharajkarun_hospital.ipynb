{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get data siriraj piyamaharajkarun hospital.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOWwbuF0nuMGU+MMzDirR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ninlawat-Ph/sentiment-analysis/blob/master/Get_data_siriraj_piyamaharajkarun_hospital.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ZyNabuWaO0",
        "colab_type": "code",
        "outputId": "c11f2d66-97ef-487a-d5c1-206ce647b651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "!pip install googletrans\n",
        "!pip install tqdm --upgrade\n",
        "!pip install twython"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n",
            "Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.45.0)\n",
            "Requirement already satisfied: twython in /usr/local/lib/python3.6/dist-packages (3.8.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from twython) (2.21.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yoVyjp3WpWE",
        "colab_type": "text"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dRbPbMKWqvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from time import time, sleep\n",
        "from random import randint\n",
        "\n",
        "# Translation\n",
        "from googletrans import Translator\n",
        "\n",
        "# Utilities\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# NLP\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import mark_negation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2DTRXQwWs-y",
        "colab_type": "code",
        "outputId": "e111ea3b-fc85-4019-aec4-f49ef4327b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# Download resources\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"vader_lexicon\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCCp7Ac-WvAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def honestdoc_comment(url):\n",
        "    \"\"\"\n",
        "    This function is to scrap data from a webboard (https://www.honestdocs.com).\n",
        "\n",
        "    INPUT\n",
        "    url : String\n",
        "      URL of the target website\n",
        "    \n",
        "    OUTPUT\n",
        "    comment : List\n",
        "      List of comments\n",
        "    score : List\n",
        "      List of rating score\n",
        "    \"\"\"\n",
        "    #create connection\n",
        "    data = requests.get(url)\n",
        "    print(\"requests code : {}\".format(data.status_code)) \n",
        "    print(\"note\\n2xx: success\\n4xx, 5xx: error\")\n",
        "    \n",
        "    #scrape comment and score\n",
        "    start_time = time() #start scraping data from page1\n",
        "    r = requests.get(url, params=dict(query=\"web scraping\",page=1)) \n",
        "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
        "    n = len(soup.find_all(\"div\",{\"class\":\"comments__content\"})) #find n of items in the page\n",
        "    \n",
        "    #extract each item\n",
        "    comment = [soup.find_all(\"div\",\n",
        "                             {\"class\":\"comments__content\"})[i].get_text().strip() for i in range(0,n)]\n",
        "    score = [soup.find_all(\"span\",\n",
        "                           {\"class\":\"stars star-rating\"})[i].attrs[\"data-score\"] for i in range(0,n)]\n",
        "    elapsed_time = time() - start_time #finish scraping data from page1\n",
        "    print(\"Time used for scraping data from page - 1 : {} s\".format(elapsed_time))\n",
        "    sleep(randint(1,3)) #mimic human behavior\n",
        "           \n",
        "    p = 2 #start scraping data from page2\n",
        "    while n > 0: #until the number of items in a page = 0\n",
        "        start_time = time() \n",
        "        r = requests.get(url, params=dict(query=\"web scraping\",page=p))\n",
        "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
        "        n = len(soup.find_all(\"div\",{\"class\":\"comments__content\"}))\n",
        "        [comment.append(soup.find_all(\"div\",\n",
        "                                      {\"class\":\"comments__content\"})[i].get_text().strip()) for i in range(0,n)]\n",
        "        [score.append(soup.find_all(\"span\",\n",
        "                                    {\"class\":\"stars star-rating\"})[i].attrs[\"data-score\"]) for i in range(0,n)]\n",
        "        elapsed_time = time() - start_time\n",
        "        print(\"Time used for scraping data from page - {} : {} s\".format(p, elapsed_time))\n",
        "        p +=1\n",
        "        sleep(randint(1,3))\n",
        "    \n",
        "    #backup data \n",
        "    pd.DataFrame({\"comment\": comment, \n",
        "                  \"score\": score}).to_csv(\"comment_\"+str(url[url.rfind(\"/\")+1:]) + \".csv\", index=False)\n",
        "    \n",
        "    return comment, score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTZY7qzYW0-8",
        "colab_type": "text"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLPKIbsgWxOZ",
        "colab_type": "code",
        "outputId": "8e2d1745-a400-4af8-d951-0733f62fa115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nts, scores = honestdoc_comment(r\"https://www.honestdocs.co/hospitals/siriraj-piyamaharajkarun-hospital\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requests code : 200\n",
            "note\n",
            "2xx: success\n",
            "4xx, 5xx: error\n",
            "Time used for scraping data from page - 1 : 0.9071540832519531 s\n",
            "Time used for scraping data from page - 2 : 0.9052798748016357 s\n",
            "Time used for scraping data from page - 3 : 0.8999674320220947 s\n",
            "Time used for scraping data from page - 4 : 0.9121425151824951 s\n",
            "Time used for scraping data from page - 5 : 0.8934996128082275 s\n",
            "Time used for scraping data from page - 6 : 0.8986654281616211 s\n",
            "Time used for scraping data from page - 7 : 0.9088301658630371 s\n",
            "Time used for scraping data from page - 8 : 0.995145320892334 s\n",
            "Time used for scraping data from page - 9 : 0.9213540554046631 s\n",
            "Time used for scraping data from page - 10 : 0.8936314582824707 s\n",
            "Time used for scraping data from page - 11 : 0.8948814868927002 s\n",
            "Time used for scraping data from page - 12 : 0.8993244171142578 s\n",
            "Time used for scraping data from page - 13 : 0.9049334526062012 s\n",
            "Time used for scraping data from page - 14 : 0.8996050357818604 s\n",
            "Time used for scraping data from page - 15 : 0.9011542797088623 s\n",
            "Time used for scraping data from page - 16 : 0.8924784660339355 s\n",
            "Time used for scraping data from page - 17 : 0.8946242332458496 s\n",
            "Time used for scraping data from page - 18 : 0.981940746307373 s\n",
            "Time used for scraping data from page - 19 : 0.8986721038818359 s\n",
            "Time used for scraping data from page - 20 : 0.9081416130065918 s\n",
            "Time used for scraping data from page - 21 : 0.8897988796234131 s\n",
            "Time used for scraping data from page - 22 : 0.9096240997314453 s\n",
            "Time used for scraping data from page - 23 : 0.9245212078094482 s\n",
            "Time used for scraping data from page - 24 : 0.8984482288360596 s\n",
            "Time used for scraping data from page - 25 : 0.9041314125061035 s\n",
            "Time used for scraping data from page - 26 : 0.8969285488128662 s\n",
            "Time used for scraping data from page - 27 : 0.8902273178100586 s\n",
            "Time used for scraping data from page - 28 : 0.9092257022857666 s\n",
            "Time used for scraping data from page - 29 : 0.9089241027832031 s\n",
            "Time used for scraping data from page - 30 : 1.0273163318634033 s\n",
            "Time used for scraping data from page - 31 : 0.9019601345062256 s\n",
            "Time used for scraping data from page - 32 : 0.910581111907959 s\n",
            "Time used for scraping data from page - 33 : 0.897892951965332 s\n",
            "Time used for scraping data from page - 34 : 0.9123051166534424 s\n",
            "Time used for scraping data from page - 35 : 1.011561632156372 s\n",
            "Time used for scraping data from page - 36 : 0.8923580646514893 s\n",
            "Time used for scraping data from page - 37 : 0.9300441741943359 s\n",
            "Time used for scraping data from page - 38 : 0.9111793041229248 s\n",
            "Time used for scraping data from page - 39 : 0.932401180267334 s\n",
            "Time used for scraping data from page - 40 : 0.9172706604003906 s\n",
            "Time used for scraping data from page - 41 : 0.9094593524932861 s\n",
            "Time used for scraping data from page - 42 : 0.9046227931976318 s\n",
            "Time used for scraping data from page - 43 : 0.8944547176361084 s\n",
            "Time used for scraping data from page - 44 : 0.9001822471618652 s\n",
            "Time used for scraping data from page - 45 : 0.9048275947570801 s\n",
            "Time used for scraping data from page - 46 : 0.9057407379150391 s\n",
            "Time used for scraping data from page - 47 : 0.9062106609344482 s\n",
            "Time used for scraping data from page - 48 : 0.9066083431243896 s\n",
            "Time used for scraping data from page - 49 : 0.8968086242675781 s\n",
            "Time used for scraping data from page - 50 : 0.9015746116638184 s\n",
            "Time used for scraping data from page - 51 : 0.9125680923461914 s\n",
            "Time used for scraping data from page - 52 : 1.0305075645446777 s\n",
            "Time used for scraping data from page - 53 : 0.8908045291900635 s\n",
            "Time used for scraping data from page - 54 : 0.9000260829925537 s\n",
            "Time used for scraping data from page - 55 : 0.9025247097015381 s\n",
            "Time used for scraping data from page - 56 : 0.8946406841278076 s\n",
            "Time used for scraping data from page - 57 : 0.9011869430541992 s\n",
            "Time used for scraping data from page - 58 : 0.9126746654510498 s\n",
            "Time used for scraping data from page - 59 : 0.9064919948577881 s\n",
            "Time used for scraping data from page - 60 : 0.9092040061950684 s\n",
            "Time used for scraping data from page - 61 : 0.9138991832733154 s\n",
            "Time used for scraping data from page - 62 : 0.9117200374603271 s\n",
            "Time used for scraping data from page - 63 : 0.9076747894287109 s\n",
            "Time used for scraping data from page - 64 : 0.9229016304016113 s\n",
            "Time used for scraping data from page - 65 : 0.9035108089447021 s\n",
            "Time used for scraping data from page - 66 : 0.8881547451019287 s\n",
            "Time used for scraping data from page - 67 : 0.9108924865722656 s\n",
            "Time used for scraping data from page - 68 : 0.8997728824615479 s\n",
            "Time used for scraping data from page - 69 : 0.8883109092712402 s\n",
            "Time used for scraping data from page - 70 : 0.9061899185180664 s\n",
            "Time used for scraping data from page - 71 : 0.908029317855835 s\n",
            "Time used for scraping data from page - 72 : 0.9005446434020996 s\n",
            "Time used for scraping data from page - 73 : 0.9050135612487793 s\n",
            "Time used for scraping data from page - 74 : 1.0141804218292236 s\n",
            "Time used for scraping data from page - 75 : 0.9188733100891113 s\n",
            "Time used for scraping data from page - 76 : 0.9038386344909668 s\n",
            "Time used for scraping data from page - 77 : 0.8889782428741455 s\n",
            "Time used for scraping data from page - 78 : 0.8995158672332764 s\n",
            "Time used for scraping data from page - 79 : 0.9190645217895508 s\n",
            "Time used for scraping data from page - 80 : 0.9086246490478516 s\n",
            "Time used for scraping data from page - 81 : 0.9228103160858154 s\n",
            "Time used for scraping data from page - 82 : 0.901761531829834 s\n",
            "Time used for scraping data from page - 83 : 0.8942770957946777 s\n",
            "Time used for scraping data from page - 84 : 0.9021966457366943 s\n",
            "Time used for scraping data from page - 85 : 0.9009568691253662 s\n",
            "Time used for scraping data from page - 86 : 0.9205658435821533 s\n",
            "Time used for scraping data from page - 87 : 0.8945436477661133 s\n",
            "Time used for scraping data from page - 88 : 0.9090545177459717 s\n",
            "Time used for scraping data from page - 89 : 0.9750082492828369 s\n",
            "Time used for scraping data from page - 90 : 0.9191064834594727 s\n",
            "Time used for scraping data from page - 91 : 0.9026064872741699 s\n",
            "Time used for scraping data from page - 92 : 0.893862247467041 s\n",
            "Time used for scraping data from page - 93 : 1.027064323425293 s\n",
            "Time used for scraping data from page - 94 : 0.9206676483154297 s\n",
            "Time used for scraping data from page - 95 : 0.8997757434844971 s\n",
            "Time used for scraping data from page - 96 : 1.0073049068450928 s\n",
            "Time used for scraping data from page - 97 : 0.9047834873199463 s\n",
            "Time used for scraping data from page - 98 : 0.8978850841522217 s\n",
            "Time used for scraping data from page - 99 : 0.9457638263702393 s\n",
            "Time used for scraping data from page - 100 : 0.9215192794799805 s\n",
            "Time used for scraping data from page - 101 : 0.9052836894989014 s\n",
            "Time used for scraping data from page - 102 : 0.9045243263244629 s\n",
            "Time used for scraping data from page - 103 : 0.8887948989868164 s\n",
            "Time used for scraping data from page - 104 : 0.8938546180725098 s\n",
            "Time used for scraping data from page - 105 : 0.8910343647003174 s\n",
            "Time used for scraping data from page - 106 : 0.9135746955871582 s\n",
            "Time used for scraping data from page - 107 : 0.8891501426696777 s\n",
            "Time used for scraping data from page - 108 : 0.9029526710510254 s\n",
            "Time used for scraping data from page - 109 : 0.8916044235229492 s\n",
            "Time used for scraping data from page - 110 : 0.9018282890319824 s\n",
            "Time used for scraping data from page - 111 : 0.9022619724273682 s\n",
            "Time used for scraping data from page - 112 : 0.8902833461761475 s\n",
            "Time used for scraping data from page - 113 : 0.9042906761169434 s\n",
            "Time used for scraping data from page - 114 : 0.9135301113128662 s\n",
            "Time used for scraping data from page - 115 : 0.911625862121582 s\n",
            "Time used for scraping data from page - 116 : 0.9251720905303955 s\n",
            "Time used for scraping data from page - 117 : 0.9020962715148926 s\n",
            "Time used for scraping data from page - 118 : 0.887732982635498 s\n",
            "Time used for scraping data from page - 119 : 1.0155200958251953 s\n",
            "Time used for scraping data from page - 120 : 0.8909318447113037 s\n",
            "Time used for scraping data from page - 121 : 0.9087393283843994 s\n",
            "Time used for scraping data from page - 122 : 0.9047584533691406 s\n",
            "Time used for scraping data from page - 123 : 0.9082527160644531 s\n",
            "Time used for scraping data from page - 124 : 0.9107682704925537 s\n",
            "Time used for scraping data from page - 125 : 0.9222800731658936 s\n",
            "Time used for scraping data from page - 126 : 0.9140093326568604 s\n",
            "Time used for scraping data from page - 127 : 0.915992021560669 s\n",
            "Time used for scraping data from page - 128 : 0.9114916324615479 s\n",
            "Time used for scraping data from page - 129 : 0.8993954658508301 s\n",
            "Time used for scraping data from page - 130 : 0.9365220069885254 s\n",
            "Time used for scraping data from page - 131 : 0.8924524784088135 s\n",
            "Time used for scraping data from page - 132 : 0.8916447162628174 s\n",
            "Time used for scraping data from page - 133 : 0.8511745929718018 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxbWn0T2dIPJ",
        "colab_type": "text"
      },
      "source": [
        "## Comment Translation The comments in this study are either Thai or English. Therefore, they should be standardized as English. Google translate API is the tool in this study AND Remove emoji in string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFF9YpmeZzlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# restore data\n",
        "data_comment_siriraj_piyamaharajkarun = pd.read_csv(\"comment_siriraj-piyamaharajkarun-hospital.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1t_ypja1dc8",
        "colab_type": "text"
      },
      "source": [
        "### Remove emoji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMSsM5OawXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transform emoji to sting\n",
        "import re\n",
        "import sys\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0qMxgCazLJ",
        "colab_type": "code",
        "outputId": "4aa60068-9c22-4e14-fdf1-6b4d1b7347ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tqdm.pandas()\n",
        "data_comment_siriraj_piyamaharajkarun[\"non_emojis\"] = data_comment_siriraj_piyamaharajkarun.progress_apply(lambda x: remove_emoji(x[\"comment\"]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 396/396 [00:00<00:00, 16667.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvfYKz9-1Au8",
        "colab_type": "text"
      },
      "source": [
        "### Translate en to thai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viDzGiQRa2RC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def th2en(comment):\n",
        "  return Translator().translate(comment, src=\"th\", dest=\"en\").text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSOxKh48a4hA",
        "colab_type": "code",
        "outputId": "b53321eb-9d5e-4069-a5da-3b0bb22b07b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "data_comment_siriraj_piyamaharajkarun[\"en\"] = data_comment_siriraj_piyamaharajkarun.progress_apply(lambda x: th2en(x[\"non_emojis\"]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 396/396 [00:47<00:00,  8.33it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2tDPUODcT-I",
        "colab_type": "code",
        "outputId": "7f83b3ec-b3da-49ac-bf87-e7dd47ada863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data_comment_siriraj_piyamaharajkarun.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>score</th>\n",
              "      <th>non_emojis</th>\n",
              "      <th>en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>วันนี้ไปเยี่ยมเพื่อนมาคะพึ่งคลอดลูก เป็นห้องรว...</td>\n",
              "      <td>5</td>\n",
              "      <td>วันนี้ไปเยี่ยมเพื่อนมาคะพึ่งคลอดลูก เป็นห้องรว...</td>\n",
              "      <td>Today I visited my friend's baby. But as well ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ผมเคยปัสสาวะบ่อยมากๆ วันนึง 20รอบ \\r\\n แล้วได้...</td>\n",
              "      <td>4</td>\n",
              "      <td>ผมเคยปัสสาวะบ่อยมากๆ วันนึง 20รอบ \\r\\n แล้วได้...</td>\n",
              "      <td>I urinate very often around day 20.\\r\\n He was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ดีมากเลยค่ะะ เคยไม่สบายเป็นไข้คุณหมอพูดจาน่ารั...</td>\n",
              "      <td>5</td>\n",
              "      <td>ดีมากเลยค่ะะ เคยไม่สบายเป็นไข้คุณหมอพูดจาน่ารั...</td>\n",
              "      <td>Good Ceaa had discomfort, fever, doctor's parl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>แม่ผมเข้ารับการรักษาอาการเส้นเลือดสมองตีบและเป...</td>\n",
              "      <td>1</td>\n",
              "      <td>แม่ผมเข้ารับการรักษาอาการเส้นเลือดสมองตีบและเป...</td>\n",
              "      <td>I've been treating cerebral thrombosis and pat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ดีมากเลยค่ะะ เคยไม่สบายเป็นไข้คุณหมอพูดจาน่ารั...</td>\n",
              "      <td>5</td>\n",
              "      <td>ดีมากเลยค่ะะ เคยไม่สบายเป็นไข้คุณหมอพูดจาน่ารั...</td>\n",
              "      <td>Good Ceaa had discomfort, fever, doctor's parl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  ...                                                 en\n",
              "0  วันนี้ไปเยี่ยมเพื่อนมาคะพึ่งคลอดลูก เป็นห้องรว...  ...  Today I visited my friend's baby. But as well ...\n",
              "1  ผมเคยปัสสาวะบ่อยมากๆ วันนึง 20รอบ \\r\\n แล้วได้...  ...  I urinate very often around day 20.\\r\\n He was...\n",
              "2  ดีมากเลยค่ะะ เคยไม่สบายเป็นไข้คุณหมอพูดจาน่ารั...  ...  Good Ceaa had discomfort, fever, doctor's parl...\n",
              "3  แม่ผมเข้ารับการรักษาอาการเส้นเลือดสมองตีบและเป...  ...  I've been treating cerebral thrombosis and pat...\n",
              "4  ดีมากเลยค่ะะ เคยไม่สบายเป็นไข้คุณหมอพูดจาน่ารั...  ...  Good Ceaa had discomfort, fever, doctor's parl...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Wjb2ba0xrV",
        "colab_type": "text"
      },
      "source": [
        "### Restore data to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3v8YmlcdghK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_comment_siriraj_piyamaharajkarun.to_csv(\"data_siriraj_piyamaharajkarun_en.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}